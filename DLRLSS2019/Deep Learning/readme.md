# 1. General introduction to neural networks by [Hugo Larochelle](http://www.dmi.usherb.ca/~larocheh/index_en.html)
### 1.1 Artifical Neurons
* **Universal Approximation Theorem**: "a single hidden layer neural network with a linear output unit can approximate any continous function arbitrarily well, given enough hidden units".  <br>
However, it doesn't mean there is a learning algorithm that can find the necessary parameters. So here comes multilayer neural network and deep neural networks.
* Multilayer neural networks <br>
  * activation function: sigmoid, tanh, relu, softmax, etc.
  * flow graph: nice propagation representation
 * Tricks in training 
  * basics: empirical risj minimization, L1 and L2 regularization, loss function (classification/regression), cross-entropy, stochastic gradient descent, backpropagation, gradients of activation functions, automatic differentiation using graph
  * **Initialization**: <br>
  

