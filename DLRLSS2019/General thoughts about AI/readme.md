#  ML for AI: What's next? by Yoshua Bengio
### 1.1 AI current: from from human-level AI
* sample complexity
* shallow and low-level 'understanding'
### 1.2 Unpromising researches towards human-level AI
* NLP based purely on text
* Generative models purely on sensary data (pixel level)
* Prior knowledge
* Unscaleble algorithms
* Theories not compatible with animal/human situation, real environment
### 1.3 Promising researches
* Example: High-level factors is much easier to generalize --> Grounded language learning, jointly learning Natural Language and a World Model --> BabyAI platform (ICLR 2019)
* Integrating system 1 and system 2, like multimodal 
<p align="center">
	<img src="http://upfrontanalytics.com/SITE/wp-content/uploads/2015/04/System-1-vs-System-2.jpg" alt="Sample"  width="250" height="200">
	<p align="center">
		<em>图片示例2</em>
	</p>
</p>

* Generative model in Latent space (high-level abstract space) --> unsupervised learning + self-supervised learning (denoising auto-encoders, BERT, Word2Vec, XLNet) --> Spatio-Temporal Deep Infomax(STDIM, ICLR2019 paper) --> generate on consciousness prior (2 levels of representation)
* Attention mechanism! attention to learn waht to memorize from past, what to predict abouth future --> predict some given very a few (soarse factor graph)
* [Represntation learning](http://www.iro.umontreal.ca/~lisa/pointeurs/TPAMISI-2012-04-0260-1.pdf)
* Unsupervised agnecy with intrisic rewards (curiosity, controllability)
* Transfer learning / Meta-learning, reuse, fast adaption to changes in distribution
* Breaking knowledge into recomposable pieces:reusable pirces
* Causality, correct causal structure leads to faster adaption --> turn nonstationarities in distribution into factorize knowledge to maximize fast transfer [paper](https://kopernio.com/viewer?doi=arXiv:1901.10912&route=6)
### 1.4 AI and society


# AI thoughts by Richard Sutton
